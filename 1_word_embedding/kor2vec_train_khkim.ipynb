{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kor2vec train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from konlpy.tag import Twitter\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(train_text, min_count, sampling_rate):\n",
    "    words = list()\n",
    "    for line in desc_list:\n",
    "        sentence = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', line).strip().split()\n",
    "        if sentence:\n",
    "            words.append(sentence)\n",
    "\n",
    "    word_counter = [['UNK', -1]]\n",
    "    word_counter.extend(collections.Counter([word for sentence in words for word in sentence]).most_common())\n",
    "    word_counter = [item for item in word_counter if item[1] >= min_count or item[0] == 'UNK']\n",
    "\n",
    "    word_list = list()\n",
    "    word_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        word_list.append(word) # 학습에 사용된 word를 저장한다. (visualize를 위해)\n",
    "        word_dict[word] = len(word_dict)\n",
    "    word_reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    word_to_pos_li = dict()\n",
    "    pos_list = list()\n",
    "    twitter = Twitter()\n",
    "    for w in word_dict:\n",
    "        w_pos_li = list()\n",
    "        for pos in twitter.pos(w, norm=True):\n",
    "            w_pos_li.append(pos)\n",
    "\n",
    "        word_to_pos_li[word_dict[w]] = w_pos_li\n",
    "        pos_list += w_pos_li\n",
    "\n",
    "    pos_counter = collections.Counter(pos_list).most_common()\n",
    "\n",
    "    pos_dict = dict()\n",
    "    for pos, _ in pos_counter:\n",
    "        pos_dict[pos] = len(pos_dict)\n",
    "\n",
    "    pos_reverse_dict = dict(zip(pos_dict.values(), pos_dict.keys()))\n",
    "\n",
    "    word_to_pos_dict = dict()\n",
    "\n",
    "    for word_id, pos_li in word_to_pos_li.items():\n",
    "        pos_id_li = list()\n",
    "        for pos in pos_li:\n",
    "            pos_id_li.append(pos_dict[pos])\n",
    "        word_to_pos_dict[word_id] = pos_id_li\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for sentence in words:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = word_dict['UNK']\n",
    "                unk_count += 1\n",
    "            s.append(index)\n",
    "        data.append(s)\n",
    "    word_counter[0][1] = max(1, unk_count)\n",
    "\n",
    "    # data = sub_sampling(data, word_counter, word_dict, sampling_rate)\n",
    "\n",
    "    return data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list\n",
    "\n",
    "def sub_sampling(data, word_counter, word_dict, sampling_rate):\n",
    "    total_words = sum([len(sentence) for sentence in data])\n",
    "    # print(\"total_words: {}\".format(total_words))\n",
    "    prob_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        f = count / total_words # 빈도수가 많을수록 f가 1에 가까워짐.\n",
    "        p = max(0, 1 - math.sqrt(sampling_rate / f)) # sampling_rate가 0.0001이면 f가 클수록 prob이 커진다.\n",
    "        prob_dict[word_dict[word]] = p\n",
    "        # print(\"count : {}, f : {}, p : {}, prob_dict : {}\".format(count, f, p, prob_dict))\n",
    "\n",
    "    new_data = list()\n",
    "    for sentence in data:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            prob = prob_dict[word]\n",
    "            if random.random() > prob: # prob이 작을수록 s에 저장되기 쉬움.\n",
    "                s.append(word)\n",
    "        new_data.append(s)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crawling한 데이터를 불러온다.\n",
    "pk_data = pd.read_csv(DATA_PATH + 'pk_data_g1.csv')\n",
    "desc_list = []\n",
    "for i in range(len(pk_data)):\n",
    "    for desc in pk_data['desc'][i].split('.'):\n",
    "        desc_list.append(desc)\n",
    "\n",
    "sampling_rate = 0.0001\n",
    "min_count = 5\n",
    "\n",
    "data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list \\\n",
    "        = build_dataset(desc_list, min_count, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습에 사용된 word list 저장\n",
    "f = open(\"word_list.txt\", 'w')\n",
    "for word in word_list:\n",
    "    input_word = \"{} \".format(word)\n",
    "    f.write(input_word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences : 4799\n",
      "vocabulary size : 1660\n",
      "pos size : 1335\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word_dict)\n",
    "pos_size = len(pos_dict)\n",
    "num_sentences = len(data)\n",
    "\n",
    "print(\"number of sentences :\", num_sentences)\n",
    "print(\"vocabulary size :\", vocabulary_size)\n",
    "print(\"pos size :\", pos_size)\n",
    "\n",
    "pos_li = []\n",
    "for key in sorted(pos_reverse_dict):\n",
    "    pos_li.append(pos_reverse_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function to generate a training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "batch_size = 150\n",
    "\n",
    "# kor2vec 의 input index list와 output index list를 만든다.\n",
    "# 윈도우 사이즈에 따라 input output pair가 늘어난다.(input이 중복)\n",
    "def generate_input_output_list(data, window_size):\n",
    "    input_li = list()\n",
    "    output_li = list()\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    if sentence[i]!=word_dict['UNK'] and sentence[j]!=word_dict['UNK']:\n",
    "                        input_li.append(sentence[i])\n",
    "                        output_li.append(sentence[j])\n",
    "    return input_li, output_li\n",
    "\n",
    "input_li, output_li = generate_input_output_list(data, window_size)\n",
    "input_li_size = len(input_li)\n",
    "\n",
    "# 확인\n",
    "# for i in range(input_li_size):\n",
    "#     print(\"-{}-\".format(i)) \n",
    "#     in_index = word_to_pos_dict[input_li[i]]\n",
    "#     out_index = word_to_pos_dict[output_li[i]]\n",
    "#     print(in_index)\n",
    "#     for ind in in_index:\n",
    "#         print(pos_reverse_dict[ind])\n",
    "#     print(out_index)\n",
    "#     for o in out_index:\n",
    "#         print(pos_reverse_dict[o])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "81434\n",
      "(150,)\n",
      "[ 426  426  426  426  426  282  282  282  282  282  282   59   59   59\n",
      "   59   59   59   59  558  558  558  558  558  558  558  558  874  874\n",
      "  874  874  874  874  874  874   60   60   60   60   60   60   60   60\n",
      "  875  875  875  875  875  875  875   55   55   55   55   55   55  184\n",
      "  184  184  184  184  634  634  634  634  282  282  282  282  282   59\n",
      "   59   59   59   59   59  104  104  104  104  104  104  104  874  874\n",
      "  874  874  874  874  874  427  427  427  427  427  427  427  876  876\n",
      "  876  876  876  876  428  428  428  428  428    3    3    3   91   91\n",
      "   91 1036 1036 1036  726  726  726  727  727  727   58   58   58   58\n",
      " 1299 1299 1299 1299   62   62   62   62 1300 1300 1300 1300  383  383\n",
      "  383  383   58   58 1301 1301 1300 1300  876  876]\n",
      "(150, 1)\n",
      "[[ 282]\n",
      " [  59]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [  60]\n",
      " [ 426]\n",
      " [  59]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [  60]\n",
      " [ 875]\n",
      " [ 426]\n",
      " [ 282]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [  60]\n",
      " [ 875]\n",
      " [  55]\n",
      " [ 426]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 874]\n",
      " [  60]\n",
      " [ 875]\n",
      " [  55]\n",
      " [ 184]\n",
      " [ 426]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 558]\n",
      " [  60]\n",
      " [ 875]\n",
      " [  55]\n",
      " [ 184]\n",
      " [ 426]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [ 875]\n",
      " [  55]\n",
      " [ 184]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [  60]\n",
      " [  55]\n",
      " [ 184]\n",
      " [  59]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [  60]\n",
      " [ 875]\n",
      " [ 184]\n",
      " [ 558]\n",
      " [ 874]\n",
      " [  60]\n",
      " [ 875]\n",
      " [  55]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 104]\n",
      " [ 874]\n",
      " [ 634]\n",
      " [  59]\n",
      " [ 104]\n",
      " [ 874]\n",
      " [ 427]\n",
      " [ 634]\n",
      " [ 282]\n",
      " [ 104]\n",
      " [ 874]\n",
      " [ 427]\n",
      " [ 876]\n",
      " [ 634]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 874]\n",
      " [ 427]\n",
      " [ 876]\n",
      " [ 428]\n",
      " [ 634]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 104]\n",
      " [ 427]\n",
      " [ 876]\n",
      " [ 428]\n",
      " [ 282]\n",
      " [  59]\n",
      " [ 104]\n",
      " [ 874]\n",
      " [ 876]\n",
      " [ 428]\n",
      " [   3]\n",
      " [  59]\n",
      " [ 104]\n",
      " [ 874]\n",
      " [ 427]\n",
      " [ 428]\n",
      " [   3]\n",
      " [ 104]\n",
      " [ 874]\n",
      " [ 427]\n",
      " [ 876]\n",
      " [   3]\n",
      " [ 427]\n",
      " [ 876]\n",
      " [ 428]\n",
      " [1036]\n",
      " [ 726]\n",
      " [ 727]\n",
      " [  91]\n",
      " [ 726]\n",
      " [ 727]\n",
      " [  91]\n",
      " [1036]\n",
      " [ 727]\n",
      " [  91]\n",
      " [1036]\n",
      " [ 726]\n",
      " [1299]\n",
      " [  62]\n",
      " [1300]\n",
      " [ 383]\n",
      " [  58]\n",
      " [  62]\n",
      " [1300]\n",
      " [ 383]\n",
      " [  58]\n",
      " [1299]\n",
      " [1300]\n",
      " [ 383]\n",
      " [  58]\n",
      " [1299]\n",
      " [  62]\n",
      " [ 383]\n",
      " [  58]\n",
      " [1299]\n",
      " [  62]\n",
      " [1300]\n",
      " [1301]\n",
      " [1300]\n",
      " [  58]\n",
      " [1300]\n",
      " [  58]\n",
      " [1301]\n",
      " [1302]\n",
      " [   1]]\n",
      "[[523], [523], [523], [523], [523], [19, 208], [19, 208], [19, 208], [19, 208], [19, 208], [19, 208], [33, 5], [33, 5], [33, 5], [33, 5], [33, 5], [33, 5], [33, 5], [596, 3], [596, 3], [596, 3], [596, 3], [596, 3], [596, 3], [596, 3], [596, 3], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [101], [101], [101], [101], [101], [101], [101], [101], [785], [785], [785], [785], [785], [785], [785], [356], [356], [356], [356], [356], [356], [191, 0], [191, 0], [191, 0], [191, 0], [191, 0], [264, 39], [264, 39], [264, 39], [264, 39], [19, 208], [19, 208], [19, 208], [19, 208], [19, 208], [33, 5], [33, 5], [33, 5], [33, 5], [33, 5], [33, 5], [379], [379], [379], [379], [379], [379], [379], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [293, 2], [105, 230], [105, 230], [105, 230], [105, 230], [105, 230], [105, 230], [105, 230], [22, 153], [22, 153], [22, 153], [22, 153], [22, 153], [22, 153], [524], [524], [524], [524], [524], [13, 0], [13, 0], [13, 0], [175], [175], [175], [272, 888, 16], [272, 888, 16], [272, 888, 16], [691], [691], [691], [266, 64], [266, 64], [266, 64], [33, 3], [33, 3], [33, 3], [33, 3], [293, 5], [293, 5], [293, 5], [293, 5], [359], [359], [359], [359], [128, 2], [128, 2], [128, 2], [128, 2], [505, 0], [505, 0], [505, 0], [505, 0], [33, 3], [33, 3], [60, 26], [60, 26], [128, 2], [128, 2], [22, 153], [22, 153]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "print(input_li_size)\n",
    "def generate_batch(iter, batch_size, input_li, output_li):\n",
    "    index = (iter % (input_li_size//batch_size)) * batch_size\n",
    "    batch_input = input_li[index:index+batch_size]\n",
    "    batch_output_li = output_li[index:index+batch_size]\n",
    "    batch_output = [[i] for i in batch_output_li]\n",
    "\n",
    "    return np.array(batch_input), np.array(batch_output)\n",
    "\n",
    "batch_inputs, batch_labels = generate_batch(0, batch_size, input_li, output_li)\n",
    "print(np.shape(batch_inputs))\n",
    "print(batch_inputs)\n",
    "print(np.shape(batch_labels))\n",
    "print(batch_labels)\n",
    "word_list = []\n",
    "for word in batch_inputs:\n",
    "    word_list.append(word_to_pos_dict[word])\n",
    "print(word_list)\n",
    "#     for pos in word_to_pos_dict[word]:\n",
    "#         print(pos)\n",
    "#         print(pos_reverse_dict[pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-2e0517cdf3c3>:48: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 150\n",
    "num_sampled = 50\n",
    "learning_rate = 1.0\n",
    "\n",
    "valid_size = 20     # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False) # 200까지 숫자 중에서 랜덤하게 20개 뽑음\n",
    "\n",
    "# tensorflow 신경망 모델 그래프 생성\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    words_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(batch_size)] # batch_size만큼의 word를 형태소로\n",
    "    vocabulary_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(vocabulary_size)] # word_dict만큼의 word를 형태소로.. 인거 같은데 안씀\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # \"/device:GPU:0\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        # embedding vector -> 우리가 원하는 최종 출력\n",
    "        pos_embeddings = tf.Variable(tf.random_uniform([pos_size, embedding_size], -1.0, 1.0), name='pos_embeddings')\n",
    "\n",
    "        word_vec_list = []\n",
    "        for i in range(batch_size):\n",
    "            word_vec = tf.reduce_sum(tf.nn.embedding_lookup(pos_embeddings, words_matrix[i]), 0)\n",
    "            word_vec_list.append(word_vec)\n",
    "        word_embeddings = tf.stack(word_vec_list) # word의 각 형태소를 embedding한 vector\n",
    "    \n",
    "        # Noise-Contrastive Estimation\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), name='nce_weights'\n",
    "        )\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name='nce_biases')\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=word_embeddings,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Compute the cosine similarity between minibatch exaples and all embeddings.\n",
    "    # 임의의 word로 유사도 검증\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(pos_embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = pos_embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations for each epoch : 542\n",
      "Initialized - Tensorflow\n",
      "Average loss at step  0 :  145.46214294433594\n",
      "Nearest to ('독침', 'Noun'): ('팬텀', 'Noun'), ('날아다니', 'Verb'), ('이끼', 'Noun'), ('인', 'Josa'), ('보인', 'Verb'), ('온몸', 'Noun'), ('지금', 'Noun'), ('아라리', 'Noun'),\n",
      "Nearest to ('지느러미', 'Noun'): ('향해', 'Verb'), ('소유자', 'Noun'), ('구멍', 'Noun'), ('낼', 'Noun'), ('일격', 'Noun'), ('인해', 'Verb'), ('보호하고', 'Verb'), ('울', 'PreEomi'),\n",
      "Nearest to ('전', 'Noun'): ('둥지', 'Noun'), ('강력하다', 'Adjective'), ('치는', 'Verb'), ('정밀', 'Noun'), ('안전한', 'Adjective'), ('가진', 'Verb'), ('추위', 'Noun'), ('이지만', 'Josa'),\n",
      "Nearest to ('진화', 'Noun'): ('사', 'Verb'), ('깊이', 'Noun'), ('힘', 'Noun'), ('발견되', 'Verb'), ('눈', 'Noun'), ('양은', 'Noun'), ('꾸고', 'Verb'), ('까지', 'Noun'),\n",
      "Nearest to ('빠르', 'Adjective'): ('은', 'Eomi'), ('많이', 'Adverb'), ('것', 'Noun'), ('쉴', 'Verb'), ('잡는', 'Verb'), ('지어', 'Verb'), ('뇌', 'Noun'), ('시킨', 'Verb'),\n",
      "Nearest to ('뿔', 'Noun'): ('후', 'Noun'), ('가끔', 'Noun'), ('뿌린', 'Verb'), ('어미', 'Noun'), ('적은', 'Verb'), ('달', 'Noun'), ('한층', 'Noun'), ('무거워', 'Adjective'),\n",
      "Nearest to ('몸', 'Noun'): ('도망간', 'Verb'), ('찾아온', 'Verb'), ('거의', 'Noun'), ('춤', 'Noun'), ('흑점', 'Noun'), ('파', 'Noun'), ('숨는', 'Verb'), ('알레르기', 'Noun'),\n",
      "Nearest to ('에너지', 'Noun'): ('교대', 'Noun'), ('서', 'Josa'), ('한', 'Josa'), ('차게', 'Verb'), ('너무', 'Noun'), ('몇', 'Noun'), ('정밀', 'Noun'), ('따라', 'Verb'),\n",
      "Nearest to ('눈', 'Noun'): ('난폭', 'Noun'), ('오랫동안', 'Noun'), ('상징', 'Noun'), ('그러나', 'Conjunction'), ('뜨', 'Verb'), ('동료', 'Noun'), ('털은', 'Verb'), ('사용하는', 'Verb'),\n",
      "Nearest to ('기절', 'Noun'): ('썩은', 'Verb'), ('밸런스', 'Noun'), ('생활한', 'Verb'), ('잎사귀', 'Noun'), ('마음', 'Noun'), ('나타나지', 'Verb'), ('서식', 'Noun'), ('풀밭', 'Noun'),\n",
      "Nearest to ('하다', 'Verb'): ('중력', 'Noun'), ('리듬', 'Noun'), ('한', 'Determiner'), ('붉은', 'Adjective'), ('방향', 'Noun'), ('커서', 'Noun'), ('과', 'Josa'), ('마력', 'Noun'),\n",
      "Nearest to ('혀', 'Noun'): ('뿐', 'Noun'), ('해골', 'Noun'), ('쓰기', 'Noun'), ('면', 'Josa'), ('하여', 'Verb'), ('해', 'Noun'), ('하지만', 'Conjunction'), ('떨어질', 'Verb'),\n",
      "Nearest to ('소리', 'Noun'): ('나타나서', 'Verb'), ('있게', 'Adjective'), ('생물', 'Noun'), ('쬐어', 'Noun'), ('똑바로', 'Noun'), ('기억하고', 'Verb'), ('하게', 'Verb'), ('안은', 'Verb'),\n",
      "Nearest to ('안', 'Noun'): ('녹이는', 'Verb'), ('별로', 'Noun'), ('모여든', 'Verb'), ('태양', 'Noun'), ('이든', 'Josa'), ('가득하다', 'Adjective'), ('인분', 'Noun'), ('수분', 'Noun'),\n",
      "Nearest to ('km', 'Alpha'): ('독가스', 'Noun'), ('될', 'Verb'), ('높이', 'Noun'), ('땅', 'Noun'), ('입', 'Noun'), ('가는', 'Verb'), ('세우고', 'Verb'), ('찾아온', 'Verb'),\n",
      "Nearest to ('촉수', 'Noun'): ('온몸', 'Noun'), ('온도', 'Noun'), ('내며', 'Verb'), ('부드러운', 'Adjective'), ('단', 'Verb'), ('스피드', 'Noun'), ('될', 'Verb'), ('떨어질', 'Verb'),\n",
      "Nearest to ('체온', 'Noun'): ('두르', 'Verb'), ('기척', 'Noun'), ('유전자', 'Noun'), ('단단하다', 'Adjective'), ('베', 'Verb'), ('파서', 'Noun'), ('커다란', 'Adjective'), ('있', 'PreEomi'),\n",
      "Nearest to ('련', 'Eomi'): ('영역', 'Noun'), ('염', 'Noun'), ('포자', 'Noun'), ('여러', 'Noun'), ('갑옷', 'Noun'), ('끝', 'Noun'), ('지', 'Josa'), ('상대', 'Noun'),\n",
      "Nearest to ('가스', 'Noun'): ('이해하는', 'Verb'), ('부드럽', 'Adjective'), ('추측', 'Noun'), ('숨어', 'Verb'), ('움직', 'Noun'), ('세밀', 'Noun'), ('가시', 'Noun'), ('변화했', 'Verb'),\n",
      "Nearest to ('어', 'Eomi'): ('동그란', 'Adjective'), ('지하', 'Noun'), ('한층', 'Noun'), ('보이', 'Noun'), ('모으는', 'Verb'), ('능력', 'Noun'), ('알파파', 'Noun'), ('속이', 'Verb'),\n",
      "Average loss at step  542 :  5.750773530006409\n",
      "Average loss at step  1084 :  1.4304582916498185\n",
      "Nearest to ('독침', 'Noun'): ('팬텀', 'Noun'), ('주', 'PreEomi'), ('날아다니', 'Verb'), ('온몸', 'Noun'), ('이끼', 'Noun'), ('강력한', 'Adjective'), ('인', 'Josa'), ('맹렬', 'Noun'),\n",
      "Nearest to ('지느러미', 'Noun'): ('소유자', 'Noun'), ('향해', 'Verb'), ('구멍', 'Noun'), ('낼', 'Noun'), ('일격', 'Noun'), ('인해', 'Verb'), ('언제나', 'Adverb'), ('힘든', 'Adjective'),\n",
      "Nearest to ('전', 'Noun'): ('강력하다', 'Adjective'), ('정밀', 'Noun'), ('불안', 'Noun'), ('둥지', 'Noun'), ('가진', 'Verb'), ('구성해서', 'Verb'), ('내서', 'Verb'), ('일으킬', 'Verb'),\n",
      "Nearest to ('진화', 'Noun'): ('깊이', 'Noun'), ('꾸고', 'Verb'), ('튀어', 'Noun'), ('찔', 'Verb'), ('아름다운', 'Adjective'), ('사', 'Verb'), ('뿌려', 'Verb'), ('너무', 'Noun'),\n",
      "Nearest to ('빠르', 'Adjective'): ('은', 'Eomi'), ('많이', 'Adverb'), ('쉴', 'Verb'), ('것', 'Noun'), ('가득하다', 'Adjective'), ('잡는', 'Verb'), ('지어', 'Verb'), ('뇌', 'Noun'),\n",
      "Nearest to ('뿔', 'Noun'): ('후', 'Noun'), ('가끔', 'Noun'), ('적은', 'Verb'), ('한층', 'Noun'), ('무거워', 'Adjective'), ('뿌린', 'Verb'), ('상냥한', 'Adjective'), ('굉장한', 'Adjective'),\n",
      "Nearest to ('몸', 'Noun'): ('향한', 'Verb'), ('자라면', 'Verb'), ('더러운', 'Adjective'), ('도의', 'Noun'), ('파', 'Noun'), ('떨어진', 'Verb'), ('갉아', 'Noun'), ('처음', 'Noun'),\n",
      "Nearest to ('에너지', 'Noun'): ('서', 'Josa'), ('게다가', 'Noun'), ('한', 'Josa'), ('너무', 'Noun'), ('교대', 'Noun'), ('따라', 'Verb'), ('하는', 'Verb'), ('차게', 'Verb'),\n",
      "Nearest to ('눈', 'Noun'): ('오랫동안', 'Noun'), ('상징', 'Noun'), ('그러나', 'Conjunction'), ('난폭', 'Noun'), ('털은', 'Verb'), ('얕은', 'Adjective'), ('대로', 'Josa'), ('사용하는', 'Verb'),\n",
      "Nearest to ('기절', 'Noun'): ('썩은', 'Verb'), ('밸런스', 'Noun'), ('풀밭', 'Noun'), ('잎사귀', 'Noun'), ('나타나지', 'Verb'), ('생활한', 'Verb'), ('마음', 'Noun'), ('아름답', 'Adjective'),\n",
      "Nearest to ('하다', 'Verb'): ('붉은', 'Adjective'), ('과', 'Josa'), ('재', 'Noun'), ('커서', 'Noun'), ('같이', 'Josa'), ('한', 'Determiner'), ('사용하여', 'Verb'), ('손가락', 'Noun'),\n",
      "Nearest to ('혀', 'Noun'): ('쓰기', 'Noun'), ('하여', 'Verb'), ('해', 'Noun'), ('하지만', 'Conjunction'), ('면', 'Josa'), ('해골', 'Noun'), ('떨어질', 'Verb'), ('뿐', 'Noun'),\n",
      "Nearest to ('소리', 'Noun'): ('있게', 'Adjective'), ('나타나서', 'Verb'), ('쬐어', 'Noun'), ('생물', 'Noun'), ('않고', 'Verb'), ('똑바로', 'Noun'), ('기억하고', 'Verb'), ('일이', 'Noun'),\n",
      "Nearest to ('안', 'Noun'): ('가득하다', 'Adjective'), ('녹이는', 'Verb'), ('별로', 'Noun'), ('달린', 'Verb'), ('태양', 'Noun'), ('모여든', 'Verb'), ('굉장하다', 'Adjective'), ('자주', 'Noun'),\n",
      "Nearest to ('km', 'Alpha'): ('독가스', 'Noun'), ('될', 'Verb'), ('땅', 'Noun'), ('구성해서', 'Verb'), ('높이', 'Noun'), ('갉아', 'Noun'), ('가는', 'Verb'), ('꿀', 'Noun'),\n",
      "Nearest to ('촉수', 'Noun'): ('온몸', 'Noun'), ('내며', 'Verb'), ('될', 'Verb'), ('부드러운', 'Adjective'), ('온도', 'Noun'), ('떨어질', 'Verb'), ('스피드', 'Noun'), ('없', 'Adjective'),\n",
      "Nearest to ('체온', 'Noun'): ('두르', 'Verb'), ('커다란', 'Adjective'), ('기척', 'Noun'), ('유전자', 'Noun'), ('단단하다', 'Adjective'), ('파서', 'Noun'), ('평생', 'Noun'), ('자유로이', 'Adverb'),\n",
      "Nearest to ('련', 'Eomi'): ('영역', 'Noun'), ('여러', 'Noun'), ('갑옷', 'Noun'), ('포자', 'Noun'), ('원래', 'Noun'), ('끝', 'Noun'), ('염', 'Noun'), ('내는', 'Verb'),\n",
      "Nearest to ('가스', 'Noun'): ('이해하는', 'Verb'), ('숨어', 'Verb'), ('가시', 'Noun'), ('부드럽', 'Adjective'), ('움직', 'Noun'), ('건조한', 'Adjective'), ('추측', 'Noun'), ('전기', 'Noun'),\n",
      "Nearest to ('어', 'Eomi'): ('동그란', 'Adjective'), ('1700', 'Number'), ('지하', 'Noun'), ('한층', 'Noun'), ('불안', 'Noun'), ('모으는', 'Verb'), ('적다', 'Verb'), ('속이', 'Verb'),\n",
      "Average loss at step  1626 :  1.2203739578723907\n",
      "Average loss at step  2168 :  1.11250836789608\n",
      "Average loss at step  2710 :  1.0358052701354026\n",
      "Nearest to ('독침', 'Noun'): ('주', 'PreEomi'), ('이끼', 'Noun'), ('팬텀', 'Noun'), ('않게', 'Verb'), ('강력한', 'Adjective'), ('온몸', 'Noun'), ('아라리', 'Noun'), ('인', 'Josa'),\n",
      "Nearest to ('지느러미', 'Noun'): ('소유자', 'Noun'), ('향해', 'Verb'), ('낼', 'Noun'), ('인해', 'Verb'), ('구멍', 'Noun'), ('일격', 'Noun'), ('보호하고', 'Verb'), ('힘든', 'Adjective'),\n",
      "Nearest to ('전', 'Noun'): ('강력하다', 'Adjective'), ('정밀', 'Noun'), ('둥지', 'Noun'), ('가진', 'Verb'), ('일으킬', 'Verb'), ('잠든', 'Verb'), ('내서', 'Verb'), ('만들기', 'Noun'),\n",
      "Nearest to ('진화', 'Noun'): ('깊이', 'Noun'), ('꾸고', 'Verb'), ('사', 'Verb'), ('아름다운', 'Adjective'), ('튀어', 'Noun'), ('어떠한', 'Adjective'), ('발견되', 'Verb'), ('찔', 'Verb'),\n",
      "Nearest to ('빠르', 'Adjective'): ('은', 'Eomi'), ('많이', 'Adverb'), ('것', 'Noun'), ('쉴', 'Verb'), ('잡는', 'Verb'), ('가득하다', 'Adjective'), ('발달한', 'Verb'), ('움직여', 'Verb'),\n",
      "Nearest to ('뿔', 'Noun'): ('후', 'Noun'), ('가끔', 'Noun'), ('적은', 'Verb'), ('한층', 'Noun'), ('굉장한', 'Adjective'), ('무거워', 'Adjective'), ('뿌린', 'Verb'), ('상냥한', 'Adjective'),\n",
      "Nearest to ('몸', 'Noun'): ('자라면', 'Verb'), ('도망간', 'Verb'), ('알레르기', 'Noun'), ('더러운', 'Adjective'), ('탈피', 'Noun'), ('떨어진', 'Verb'), ('찾아온', 'Verb'), ('불꽃', 'Noun'),\n",
      "Nearest to ('에너지', 'Noun'): ('서', 'Josa'), ('게다가', 'Noun'), ('담겨', 'Verb'), ('너무', 'Noun'), ('몇', 'Noun'), ('따라', 'Verb'), ('하는', 'Verb'), ('교대', 'Noun'),\n",
      "Nearest to ('눈', 'Noun'): ('오랫동안', 'Noun'), ('상징', 'Noun'), ('그러나', 'Conjunction'), ('털은', 'Verb'), ('얕은', 'Adjective'), ('사용하는', 'Verb'), ('대로', 'Josa'), ('난폭', 'Noun'),\n",
      "Nearest to ('기절', 'Noun'): ('썩은', 'Verb'), ('밸런스', 'Noun'), ('잎사귀', 'Noun'), ('아름답', 'Adjective'), ('생활한', 'Verb'), ('풀밭', 'Noun'), ('마음', 'Noun'), ('나타나지', 'Verb'),\n",
      "Nearest to ('하다', 'Verb'): ('붉은', 'Adjective'), ('과', 'Josa'), ('중력', 'Noun'), ('리듬', 'Noun'), ('손가락', 'Noun'), ('커서', 'Noun'), ('같이', 'Josa'), ('좋아해서', 'Adjective'),\n",
      "Nearest to ('혀', 'Noun'): ('하여', 'Verb'), ('쓰기', 'Noun'), ('해', 'Noun'), ('하지만', 'Conjunction'), ('면', 'Josa'), ('가', 'Josa'), ('해골', 'Noun'), ('뿐', 'Noun'),\n",
      "Nearest to ('소리', 'Noun'): ('있게', 'Adjective'), ('나타나서', 'Verb'), ('쬐어', 'Noun'), ('기억하고', 'Verb'), ('않고', 'Verb'), ('생물', 'Noun'), ('똑바로', 'Noun'), ('하게', 'Verb'),\n",
      "Nearest to ('안', 'Noun'): ('가득하다', 'Adjective'), ('녹이는', 'Verb'), ('별로', 'Noun'), ('달린', 'Verb'), ('태양', 'Noun'), ('굉장하다', 'Adjective'), ('자주', 'Noun'), ('모여든', 'Verb'),\n",
      "Nearest to ('km', 'Alpha'): ('독가스', 'Noun'), ('될', 'Verb'), ('입', 'Noun'), ('가는', 'Verb'), ('높이', 'Noun'), ('돌아올', 'Verb'), ('좋아해서', 'Adjective'), ('모인', 'Verb'),\n",
      "Nearest to ('촉수', 'Noun'): ('온몸', 'Noun'), ('될', 'Verb'), ('내며', 'Verb'), ('부드러운', 'Adjective'), ('온도', 'Noun'), ('떨어질', 'Verb'), ('스피드', 'Noun'), ('불꽃', 'Noun'),\n",
      "Nearest to ('체온', 'Noun'): ('커다란', 'Adjective'), ('두르', 'Verb'), ('자유로이', 'Adverb'), ('단단하다', 'Adjective'), ('파서', 'Noun'), ('평생', 'Noun'), ('기척', 'Noun'), ('유전자', 'Noun'),\n",
      "Nearest to ('련', 'Eomi'): ('영역', 'Noun'), ('여러', 'Noun'), ('갑옷', 'Noun'), ('포자', 'Noun'), ('끝', 'Noun'), ('을', 'Josa'), ('염', 'Noun'), ('쉽', 'Verb'),\n",
      "Nearest to ('가스', 'Noun'): ('이해하는', 'Verb'), ('가시', 'Noun'), ('숨어', 'Verb'), ('그리고', 'Conjunction'), ('찾아온', 'Verb'), ('움직', 'Noun'), ('새끼', 'Noun'), ('전자', 'Noun'),\n",
      "Nearest to ('어', 'Eomi'): ('동그란', 'Adjective'), ('지하', 'Noun'), ('한층', 'Noun'), ('1700', 'Number'), ('될', 'Verb'), ('적다', 'Verb'), ('속이', 'Verb'), ('모으는', 'Verb'),\n",
      "Average loss at step  3252 :  0.9812666937112808\n",
      "Average loss at step  3794 :  0.9422595601081848\n",
      "Nearest to ('독침', 'Noun'): ('않게', 'Verb'), ('강력한', 'Adjective'), ('이끼', 'Noun'), ('아라리', 'Noun'), ('온몸', 'Noun'), ('주', 'PreEomi'), ('적의', 'Noun'), ('팬텀', 'Noun'),\n",
      "Nearest to ('지느러미', 'Noun'): ('소유자', 'Noun'), ('낼', 'Noun'), ('향해', 'Verb'), ('구멍', 'Noun'), ('인해', 'Verb'), ('일격', 'Noun'), ('보호하고', 'Verb'), ('새로운', 'Adjective'),\n",
      "Nearest to ('전', 'Noun'): ('강력하다', 'Adjective'), ('둥지', 'Noun'), ('정밀', 'Noun'), ('만들기', 'Noun'), ('일으킬', 'Verb'), ('잠든', 'Verb'), ('가진', 'Verb'), ('내서', 'Verb'),\n",
      "Nearest to ('진화', 'Noun'): ('꾸고', 'Verb'), ('깊이', 'Noun'), ('사', 'Verb'), ('발견되', 'Verb'), ('튀어', 'Noun'), ('어떠한', 'Adjective'), ('아름다운', 'Adjective'), ('될', 'Verb'),\n",
      "Nearest to ('빠르', 'Adjective'): ('은', 'Eomi'), ('많이', 'Adverb'), ('잡는', 'Verb'), ('움직여', 'Verb'), ('쉴', 'Verb'), ('가득하다', 'Adjective'), ('발달한', 'Verb'), ('빠른', 'Adjective'),\n",
      "Nearest to ('뿔', 'Noun'): ('후', 'Noun'), ('가끔', 'Noun'), ('한층', 'Noun'), ('적은', 'Verb'), ('굉장한', 'Adjective'), ('내는', 'Verb'), ('무거워', 'Adjective'), ('뿌린', 'Verb'),\n",
      "Nearest to ('몸', 'Noun'): ('자라면', 'Verb'), ('알레르기', 'Noun'), ('탈피', 'Noun'), ('떨어진', 'Verb'), ('더러운', 'Adjective'), ('도망간', 'Verb'), ('서게', 'Verb'), ('찾아온', 'Verb'),\n",
      "Nearest to ('에너지', 'Noun'): ('서', 'Josa'), ('담겨', 'Verb'), ('게다가', 'Noun'), ('너무', 'Noun'), ('따라', 'Verb'), ('몇', 'Noun'), ('영역', 'Noun'), ('대량', 'Noun'),\n",
      "Nearest to ('눈', 'Noun'): ('오랫동안', 'Noun'), ('상징', 'Noun'), ('털은', 'Verb'), ('그러나', 'Conjunction'), ('얕은', 'Adjective'), ('난폭', 'Noun'), ('모여', 'Verb'), ('차지', 'Noun'),\n",
      "Nearest to ('기절', 'Noun'): ('썩은', 'Verb'), ('밸런스', 'Noun'), ('생활한', 'Verb'), ('잎사귀', 'Noun'), ('아름답', 'Adjective'), ('최고', 'Noun'), ('마음', 'Noun'), ('풀밭', 'Noun'),\n",
      "Nearest to ('하다', 'Verb'): ('듯', 'Noun'), ('중력', 'Noun'), ('붉은', 'Adjective'), ('손가락', 'Noun'), ('리듬', 'Noun'), ('과', 'Josa'), ('같이', 'Josa'), ('사용하여', 'Verb'),\n",
      "Nearest to ('혀', 'Noun'): ('하여', 'Verb'), ('해골', 'Noun'), ('하지만', 'Conjunction'), ('쓰기', 'Noun'), ('해', 'Noun'), ('같이', 'Josa'), ('뿐', 'Noun'), ('면', 'Josa'),\n",
      "Nearest to ('소리', 'Noun'): ('있게', 'Adjective'), ('나타나서', 'Verb'), ('쬐어', 'Noun'), ('않고', 'Verb'), ('기억하고', 'Verb'), ('생물', 'Noun'), ('똑바로', 'Noun'), ('하게', 'Verb'),\n",
      "Nearest to ('안', 'Noun'): ('가득하다', 'Adjective'), ('녹이는', 'Verb'), ('별로', 'Noun'), ('모여든', 'Verb'), ('달린', 'Verb'), ('굉장하다', 'Adjective'), ('태양', 'Noun'), ('자주', 'Noun'),\n",
      "Nearest to ('km', 'Alpha'): ('독가스', 'Noun'), ('될', 'Verb'), ('입', 'Noun'), ('가는', 'Verb'), ('높이', 'Noun'), ('들', 'Verb'), ('돌아올', 'Verb'), ('모인', 'Verb'),\n",
      "Nearest to ('촉수', 'Noun'): ('온몸', 'Noun'), ('될', 'Verb'), ('온도', 'Noun'), ('내며', 'Verb'), ('부드러운', 'Adjective'), ('떨어질', 'Verb'), ('스피드', 'Noun'), ('짧고', 'Adjective'),\n",
      "Nearest to ('체온', 'Noun'): ('커다란', 'Adjective'), ('자유로이', 'Adverb'), ('단단하다', 'Adjective'), ('파서', 'Noun'), ('평생', 'Noun'), ('유전자', 'Noun'), ('태어날', 'Verb'), ('두르', 'Verb'),\n",
      "Nearest to ('련', 'Eomi'): ('영역', 'Noun'), ('포자', 'Noun'), ('여러', 'Noun'), ('갑옷', 'Noun'), ('을', 'Josa'), ('끝', 'Noun'), ('쉽', 'Verb'), ('염', 'Noun'),\n",
      "Nearest to ('가스', 'Noun'): ('이해하는', 'Verb'), ('가시', 'Noun'), ('숨어', 'Verb'), ('그리고', 'Conjunction'), ('찾아온', 'Verb'), ('전기', 'Noun'), ('새끼', 'Noun'), ('전자', 'Noun'),\n",
      "Nearest to ('어', 'Eomi'): ('동그란', 'Adjective'), ('지하', 'Noun'), ('한층', 'Noun'), ('될', 'Verb'), ('모으는', 'Verb'), ('1700', 'Number'), ('속이', 'Verb'), ('잠든', 'Verb'),\n",
      "Average loss at step  4336 :  0.9097166513204574\n",
      "Average loss at step  4878 :  0.8840120134353637\n",
      "Average loss at step  5420 :  0.8590105665922165\n",
      "Nearest to ('독침', 'Noun'): ('않게', 'Verb'), ('죽은', 'Verb'), ('팬텀', 'Noun'), ('강력한', 'Adjective'), ('적의', 'Noun'), ('아라리', 'Noun'), ('이끼', 'Noun'), ('온몸', 'Noun'),\n",
      "Nearest to ('지느러미', 'Noun'): ('낼', 'Noun'), ('소유자', 'Noun'), ('향해', 'Verb'), ('구멍', 'Noun'), ('일격', 'Noun'), ('인해', 'Verb'), ('새로운', 'Adjective'), ('전', 'Noun'),\n",
      "Nearest to ('전', 'Noun'): ('강력하다', 'Adjective'), ('정밀', 'Noun'), ('둥지', 'Noun'), ('만들기', 'Noun'), ('일으킬', 'Verb'), ('잠든', 'Verb'), ('지느러미', 'Noun'), ('쏜다', 'Noun'),\n",
      "Nearest to ('진화', 'Noun'): ('꾸고', 'Verb'), ('깊이', 'Noun'), ('발견되', 'Verb'), ('사', 'Verb'), ('어떠한', 'Adjective'), ('튀어', 'Noun'), ('될', 'Verb'), ('목숨', 'Noun'),\n",
      "Nearest to ('빠르', 'Adjective'): ('많이', 'Adverb'), ('움직여', 'Verb'), ('잡는', 'Verb'), ('가득하다', 'Adjective'), ('쉴', 'Verb'), ('빠른', 'Adjective'), ('은', 'Eomi'), ('깨고', 'Verb'),\n",
      "Nearest to ('뿔', 'Noun'): ('후', 'Noun'), ('가끔', 'Noun'), ('한층', 'Noun'), ('굉장한', 'Adjective'), ('적은', 'Verb'), ('내는', 'Verb'), ('긴', 'Verb'), ('무거워', 'Adjective'),\n",
      "Nearest to ('몸', 'Noun'): ('알레르기', 'Noun'), ('자라면', 'Verb'), ('탈피', 'Noun'), ('도망간', 'Verb'), ('더러운', 'Adjective'), ('찾아온', 'Verb'), ('떨어진', 'Verb'), ('불꽃', 'Noun'),\n",
      "Nearest to ('에너지', 'Noun'): ('서', 'Josa'), ('담겨', 'Verb'), ('너무', 'Noun'), ('게다가', 'Noun'), ('영역', 'Noun'), ('따라', 'Verb'), ('몇', 'Noun'), ('교대', 'Noun'),\n",
      "Nearest to ('눈', 'Noun'): ('오랫동안', 'Noun'), ('상징', 'Noun'), ('털은', 'Verb'), ('그러나', 'Conjunction'), ('얕은', 'Adjective'), ('모여', 'Verb'), ('진화', 'Noun'), ('추', 'Noun'),\n",
      "Nearest to ('기절', 'Noun'): ('썩은', 'Verb'), ('밸런스', 'Noun'), ('잎사귀', 'Noun'), ('생활한', 'Verb'), ('아름답', 'Adjective'), ('마음', 'Noun'), ('최고', 'Noun'), ('나타나지', 'Verb'),\n",
      "Nearest to ('하다', 'Verb'): ('듯', 'Noun'), ('중력', 'Noun'), ('리듬', 'Noun'), ('손가락', 'Noun'), ('붉은', 'Adjective'), ('과', 'Josa'), ('마력', 'Noun'), ('한', 'Determiner'),\n",
      "Nearest to ('혀', 'Noun'): ('하여', 'Verb'), ('해골', 'Noun'), ('쓰기', 'Noun'), ('하지만', 'Conjunction'), ('같이', 'Josa'), ('해', 'Noun'), ('조용한', 'Adjective'), ('뿐', 'Noun'),\n",
      "Nearest to ('소리', 'Noun'): ('있게', 'Adjective'), ('나타나서', 'Verb'), ('쬐어', 'Noun'), ('생물', 'Noun'), ('기억하고', 'Verb'), ('않고', 'Verb'), ('똑바로', 'Noun'), ('하게', 'Verb'),\n",
      "Nearest to ('안', 'Noun'): ('녹이는', 'Verb'), ('가득하다', 'Adjective'), ('굉장하다', 'Adjective'), ('모여든', 'Verb'), ('태양', 'Noun'), ('달린', 'Verb'), ('별로', 'Noun'), ('자주', 'Noun'),\n",
      "Nearest to ('km', 'Alpha'): ('독가스', 'Noun'), ('될', 'Verb'), ('들', 'Verb'), ('입', 'Noun'), ('높이', 'Noun'), ('가는', 'Verb'), ('갑자기', 'Noun'), ('돌아올', 'Verb'),\n",
      "Nearest to ('촉수', 'Noun'): ('온몸', 'Noun'), ('될', 'Verb'), ('온도', 'Noun'), ('내며', 'Verb'), ('떨어질', 'Verb'), ('짧고', 'Adjective'), ('부드러운', 'Adjective'), ('불꽃', 'Noun'),\n",
      "Nearest to ('체온', 'Noun'): ('커다란', 'Adjective'), ('자유로이', 'Adverb'), ('단단하다', 'Adjective'), ('태어날', 'Verb'), ('파서', 'Noun'), ('기세', 'Noun'), ('유전자', 'Noun'), ('평생', 'Noun'),\n",
      "Nearest to ('련', 'Eomi'): ('영역', 'Noun'), ('포자', 'Noun'), ('갑옷', 'Noun'), ('여러', 'Noun'), ('쉽', 'Verb'), ('끝', 'Noun'), ('염', 'Noun'), ('맹독', 'Noun'),\n",
      "Nearest to ('가스', 'Noun'): ('가시', 'Noun'), ('이해하는', 'Verb'), ('숨어', 'Verb'), ('찾아온', 'Verb'), ('그리고', 'Conjunction'), ('발견된', 'Verb'), ('전자', 'Noun'), ('새끼', 'Noun'),\n",
      "Nearest to ('어', 'Eomi'): ('동그란', 'Adjective'), ('지하', 'Noun'), ('한층', 'Noun'), ('될', 'Verb'), ('능력', 'Noun'), ('모으는', 'Verb'), ('잠든', 'Verb'), ('적다', 'Verb'),\n"
     ]
    }
   ],
   "source": [
    "num_iterations = input_li_size // batch_size\n",
    "print(\"number of iterations for each epoch :\", num_iterations)\n",
    "epochs = 10\n",
    "num_steps = num_iterations * epochs + 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized - Tensorflow\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(step, batch_size, input_li, output_li)\n",
    "\n",
    "        word_list = []\n",
    "        for word in batch_inputs:\n",
    "            word_list.append(word_to_pos_dict[word])\n",
    "\n",
    "        feed_dict = {}\n",
    "        for i in range(batch_size):\n",
    "            feed_dict[words_matrix[i]] = word_list[i]\n",
    "        feed_dict[train_inputs] = batch_inputs\n",
    "        feed_dict[train_labels] = batch_labels\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % (num_steps//10) == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % (num_steps//4) == 0:\n",
    "            pos_embed = pos_embeddings.eval()\n",
    "\n",
    "            # Print nearest words\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_pos = pos_reverse_dict[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % str(valid_pos)\n",
    "                for k in range(top_k):\n",
    "                    close_word = pos_reverse_dict[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, str(close_word))\n",
    "                print(log_str)\n",
    "\n",
    "    pos_embed = pos_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to save vectors.\n",
    "def save_model(pos_list, embeddings, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(str(len(pos_list)))\n",
    "        f.write(\" \")\n",
    "        f.write(str(embedding_size))\n",
    "        f.write(\"\\n\")\n",
    "        for i in range(len(pos_list)):\n",
    "            pos = pos_list[i]\n",
    "            f.write(str(pos).replace(\"', '\", \"','\") + \" \")\n",
    "            f.write(' '.join(map(str, embeddings[i])))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save vectors\n",
    "save_model(pos_li, pos_embed, \"pos.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor17",
   "language": "python",
   "name": "tensor17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}